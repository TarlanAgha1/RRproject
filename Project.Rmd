---
title: "Bank Marketing Data - A Decision Tree Approach"
authors: Tarlan Aghayev, Wojciech Misiura
date: "2024-05-03"
output:
  html_notebook: default
  html_document: default
---

## Context

As part of Reproducible Research course which is Mandatory course for 2nd year students of Data Science and Business Analytics, we are reproducing already existing project.

Link to the project: <https://www.kaggle.com/code/shirantha/bank-marketing-data-a-decision-tree-approach>

The aim of this project is to:
1. Verify reproductibility of the code.
2. Transform code from original code written in python to R.
3. Ensure that project written in R is reproductible. Ideally code should not require any user to adjust code to own environment.
4. Enhance referenced project.

Firstly we start with loading libaries. Loop verify whether package has been installed, if not package is installed before loading. Package versions are fixed to ensure that all features work exactly the same way.

## Function to check and install packages
```{r, message=FALSE, warning=FALSE, echo=FALSE}
packages_version <- c(
  "readr" = "2.1.4", 
  "dplyr" = "1.1.2", 
  "ggplot2" = "3.4.2",
  "fastDummies" = "1.7.3",
  "rpart" = "4.1.23",
  "caret" = "6.0.94",
  "rpart.plot" = "3.1.2",
  "pROC" = "1.18.5",
  "corrplot" = "0.92",
  "RColorBrewer" = "1.1.3",
  "tibble" = "3.2.1",
  "rpart" = "4.1.23",
  "gbm" = "2.1.8",
  "utils" = "4.3.0",
  "smotefamily" = "1.4.0"
  )

check_and_install <- function(package, version) {
  if (!require(package, character.only = TRUE)) {
    package_version_install <- paste0(package, "@", version)
    remotes::install_version(package_version_install)
  }
}


mapply(check_and_install, names(packages_version), packages_version)
```

## Dataset Description
As kaggle project was created as part of challenge, it was downloaded directly from kaggle and then has been loaded from local path. We decided to go to original destination of dataaset and we download dataset straight from UCI Machine Learning Repository. To do that we used a combination of remote and utils package. Original dataset had already renamed 'deposit' column to 'y' as it is dependent variable. We decided to rename it to 'deposit' for consistency with cited code. Please note that dataset used in kaggle used limited dataset to 11k observations, we use full dataset which consists of 45k observations.

Below is official description of dataset:
"Citation Request:
  This dataset is public available for research. The details are described in [Moro et al., 2011]. 
  Please include this citation if you plan to use this database:

  [Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. 
  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS.

  Available at: [pdf] http://hdl.handle.net/1822/14838
                [bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt

1. Title: Bank Marketing

2. Sources
   Created by: Paulo Cortez (Univ. Minho) and Sérgio Moro (ISCTE-IUL) @ 2012
   
3. Past Usage:

  The full dataset was described and analyzed in:

  S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. 
  In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, 
  Portugal, October, 2011. EUROSIS.

4. Relevant Information:

   The data is related with direct marketing campaigns of a Portuguese banking institution. 
   The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, 
   in order to access if the product (bank term deposit) would be (or not) subscribed. 

   There are two datasets: 
      1) bank-full.csv with all examples, ordered by date (from May 2008 to November 2010).
      2) bank.csv with 10% of the examples (4521), randomly selected from bank-full.csv.
   The smallest dataset is provided to test more computationally demanding machine learning algorithms (e.g. SVM).

   The classification goal is to predict if the client will subscribe a term deposit (variable y).

5. Number of Instances: 45211 for bank-full.csv (4521 for bank.csv)

6. Number of Attributes: 16 + output attribute.

7. Attribute information:

   For more information, read [Moro et al., 2011].

   Input variables:
   # bank client data:
   1 - age (numeric)
   2 - job : type of job (categorical: "admin.","unknown","unemployed","management","housemaid","entrepreneur","student",
                                       "blue-collar","self-employed","retired","technician","services") 
   3 - marital : marital status (categorical: "married","divorced","single"; note: "divorced" means divorced or widowed)
   4 - education (categorical: "unknown","secondary","primary","tertiary")
   5 - default: has credit in default? (binary: "yes","no")
   6 - balance: average yearly balance, in euros (numeric) 
   7 - housing: has housing loan? (binary: "yes","no")
   8 - loan: has personal loan? (binary: "yes","no")
   # related with the last contact of the current campaign:
   9 - contact: contact communication type (categorical: "unknown","telephone","cellular") 
  10 - day: last contact day of the month (numeric)
  11 - month: last contact month of year (categorical: "jan", "feb", "mar", ..., "nov", "dec")
  12 - duration: last contact duration, in seconds (numeric)
   # other attributes:
  13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
  14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
  15 - previous: number of contacts performed before this campaign and for this client (numeric)
  16 - poutcome: outcome of the previous marketing campaign (categorical: "unknown","other","failure","success")

  Output variable (desired target):
  17 - y - has the client subscribed a term deposit? (binary: "yes","no")

8. Missing Attribute Values: None
"
## Dataset Preparation
```{r}
zip_url <- "https://archive.ics.uci.edu/static/public/222/bank+marketing.zip"
zip_file <- "bank.zip"
download.file(zip_url, zip_file)

unzip(zip_file, exdir = "bank_dataset")

csv_file_path <- "bank_dataset/bank-full.csv"
bank <- read.csv(csv_file_path, sep = ";")



bank <- bank %>% rename(deposit = y)

variables <- data.frame(
  name = colnames(bank),
  type = sapply(bank, class)
)
print(variables)

```
Once loaded, we started first with listing down the summary of downloaded dataset.


```{r}
summary(bank)
```

First step to prepare dataset for any machine learning modeling is to determine the characteristics of dataset. First step is to verify whether dataset contains any missing values. This is done by visualizing summary of missing values per each column.

```{r}
# Brief verification of dataset to determine whether there are any missing values
bank %>% summarize(across(everything(), ~sum(is.na(.))))
```
As it can be seen above, dataset does not contain any missing values, therefore there is no requirement to perform any engineering.

```{r}
summary(bank)
```


## Explatory Data Analysis

In Explatory Data Analysis chapter, aim is to identify dataset characteristics. This is done by plotting graphs.

```{r}
g_age <- ggplot(bank, aes(x = factor(0), y = age)) + 
  geom_boxplot() +
  labs(x = NULL, y = "Age") +
  theme(axis.title.x = element_blank())
g_age

g_age_dist <- ggplot(bank, aes(x = age)) + 
  geom_histogram(bins = 100, fill = "blue", color = "black") +
  ggtitle("Distribution of Age")
g_age_dist

bank_above_70 <- bank %>% 
  filter(age > 70)

print(bank_above_70)
```
As we see majority of data lies between 25 and 60 years. Distribution follows normal distribution with left skewness. Although there are few outliers above 75 (273 values out of 11162 observations) which represent 2% of total population.


```{r}
g_duration <- ggplot(bank, aes(x = factor(0), y = duration)) + 
  geom_boxplot() +
  labs(x = NULL, y = "Duration") +
  theme(axis.title.x = element_blank())
g_duration

g_duration_dist <- ggplot(bank, aes(x = duration)) + 
  geom_histogram(bins = 100, fill = "red", color = "black") +
  ggtitle("Distribution of Duration")
g_duration_dist

g_duration_dist <- ggplot(bank%>%filter(duration>1000), aes(x = age)) + 
  geom_histogram(bins = 100, fill = "red", color = "black") +
  ggtitle("Distribution of Duration")
g_duration_dist

g_duration_dist <- ggplot(bank%>%filter(duration<1000), aes(x = age)) + 
  geom_histogram(bins = 100, fill = "red", color = "black") +
  ggtitle("Distribution of Duration")
g_duration_dist
```


## Converting Categorical Data

To ensure that dataset is ready for modeling, categorical values should be converted into integer values. 

Now, we want to count how many people who made a deposit by job category. To achieve this, we have to calculate the count of people who made a deposit in a bank, grouped by job category. Code iterates through a list of job categories and counts the number of individuals who made a deposit for each category.

```{r}
bank_data <- bank


jobs <- c('management', 'blue-collar', 'technician', 'admin.', 'services',
          'retired', 'self-employed', 'student', 'unemployed', 'entrepreneur','housemaid', 'unknown')

for (j in jobs) {
  count <- sum(bank_data$deposit == "yes" & bank_data$job == j)
  cat(sprintf("%-15s : %5d\n", j, count))}
```
Then we plot graphs to visualize Age distribution by Job for Deposits Marked as Yes. We clearly can see that Students represent the youngest group without any outliers.
```{r}
bank_data_filtered <- bank_data %>% 
  filter(deposit == "yes")

p <- ggplot(bank_data_filtered, aes(x = age)) + 
  geom_histogram(bins = 10, fill = "skyblue", color = "black") + 
  facet_wrap(~ job, scales = "free_y") + 
  labs(title = "Age Distribution by Job for Deposits Marked 'Yes'",
       x = "Age",
       y = "Count") +
  theme_minimal()
print(p)

```


```{r}
table(bank_data$job)
```

As some jobs are grouped into groups such us blue-collar, we categorizes the job types in the 'bank_data' dataset into groups such us: 'white-collar', 'pink-collar', and 'other' where,

white-collar category includes management and admin
pink-collar category includes services and housemaid
other category includes retired, student, unemployed and unknown

```{r}
bank_data$job[bank_data$job %in% c('management', 'admin.')] <- 'white-collar'

bank_data$job[bank_data$job %in% c('services', 'housemaid')] <- 'pink-collar'

bank_data$job[bank_data$job %in% c('retired', 'student', 'unemployed', 'unknown')] <- 'other'

```

Here is a brief visualization of job distribution after grouping them.
```{r}
table(bank_data$job)
```

#### ------------------------------ poutcome ------------------------------

In the dataset there are 4 types of answers - unknown, other, failure and success. As there is no difference between unknown and other for the model, we have a code to overwrite all other values to unknown.

```{r}
bank_data$poutcome <- as.character(bank_data$poutcome)

bank_data$poutcome[bank_data$poutcome == "other"] <- "unknown"

table(bank_data$poutcome)

```

#### contact 
In original project, owner removed contact from the dataset.

```{r}
bank_data <- select(bank_data, -contact)
```

#### default and housing and loan 

Then we changed categorical variables to integers which can be then further used in ml.

```{r}
bank_data$default_cat <- as.integer(bank_data$default == "yes")
bank_data$housing_cat <- as.integer(bank_data$housing == "yes")
bank_data$loan_cat <- as.integer(bank_data$loan == "yes")

bank_data <- select(bank_data, -c(default, housing, loan))
```

#### month, day 

```{r}
bank_data <- select(bank_data, -c(month, day))
```

#### ------------------------------ deposit ------------------------------

```{r}
bank_data$deposit_cat <- as.integer(bank_data$deposit == "yes")
bank_data <- select(bank_data, -deposit)
```

#### ------------------------------ pdays ------------------------------

This code segment performs the following tasks:
 1. Counts the number of customers that have not been contacted before.
 2. Finds the maximum value of the 'pdays' variable in the 'bank_data' dataset.
 3. Replaces the values of -1 in the 'pdays' variable with 10000.
 4. Creates a new variable 'recent_pdays' which is the reciprocal of 'pdays' if 'pdays' is           greater than 0, otherwise it is set to 1.
 5. Removes the 'pdays' variable from the 'bank_data' dataset.
 
```{r}
cat("Customers that have not been contacted before:", sum(bank_data$pdays == -1), "\n")
cat("Maximum values on pdays:", max(bank_data$pdays), "\n")

bank_data$pdays[bank_data$pdays == -1] <- 10000

bank_data$recent_pdays <- ifelse(bank_data$pdays > 0, 1/bank_data$pdays, 1/bank_data$pdays)
bank_data <- select(bank_data, -pdays)
```

```{r}
tail(bank_data)
```

### ------------------------------ Convert to dummy values ------------------------------

```{r}
bank_with_dummies <- dummy_cols(bank_data, select_columns = c('job', 'marital', 'education', 'poutcome'),
                                remove_selected_columns = TRUE)
head(bank_with_dummies)
```

```{r}
print(dim(bank_with_dummies))

summary(bank_with_dummies)
```
### Observations on whole population

```{r}
ggplot(bank_with_dummies, aes(x = age, y = balance)) +
  geom_point(alpha = 0.5) +
  labs(title = "Scatterplot of Age vs. Balance",
       subtitle = "Across all ages, majority of people have savings of less than 20000",
       x = "Age", y = "Balance") +
  theme_minimal()
```

```{r}
ggplot(bank_with_dummies, aes(x = duration)) + 
  geom_histogram(data = subset(bank_with_dummies, poutcome_success == 1), fill = "blue", bins = 30) +
  labs(title = "Histogram of Duration for 'poutcome_success'",
       x = "Duration", y = "Count") +
  theme_minimal()
```

### Analysis on people who sign up for a term deposite

```{r}
bank_with_dummies %>% 
  filter(deposit_cat == 1) %>%
  summary()
```

```{r}
sum(bank_with_dummies$deposit_cat == 1 & bank_with_dummies$loan_cat == 1 & bank_with_dummies$housing_cat == 1)
```

```{r}
sum(bank_with_dummies$deposit_cat == 1 & bank_with_dummies$default_cat == 1)
```

```{r}
ggplot(data = bank_data, aes(x = job, y = deposit_cat, fill = job)) +
  geom_bar(stat = "summary", fun = mean) +
  labs(title = "Average Deposit Category by Job", x = "Job Category", y = "Average Deposit Category") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  
```


```{r}
ggplot(data = bank_data, aes(x = poutcome, y = duration, fill = poutcome)) +
  geom_bar(stat = "summary", fun = mean) +
  labs(title = "Average Duration by Outcome", x = "Outcome", y = "Average Duration") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  
```

> ## Classification

```{r}
bankcl <- bank_with_dummies
```

```{r}
corr <- cor(bankcl, use = "complete.obs") 
print(corr)
```

```{r}
corrplot(corr, method = "color", type = "upper", order = "hclust",
         tl.col = "black", tl.srt = 45,
         col = colorRampPalette(c(brewer.pal(10, "RdBu")[10:1]))(200),
         title = "Heatmap of Correlation Matrix", 
         cl.lim = c(-0.3, 0.3))
```



```{r}
corr_deposit <- as.data.frame(corr['deposit_cat', , drop = FALSE])

corr_deposit <- corr_deposit[-which(rownames(corr_deposit) == 'deposit_cat'), ]

corr_deposit <- corr_deposit[order(corr_deposit$deposit_cat, decreasing = TRUE), ]

```


> ## Build the Data Model

We have set the seed to enable reproductibility of code.
```{r}
set.seed(50) 

```

We then verified the balance of the dataset

```{r}
table(bankcl$deposit_cat)
```
As dataset is imbalanced, with 88% of observations representing yes value and only 12% representing no value, we balanced out datast using majority class technique. After applying it, we see that number of observation is equal for both outcomes. We have balanced out only train data to not influence test data.

```{r}
label <- bankcl$deposit_cat

data_drop_deposite <- bankcl[, !names(bankcl) %in% 'deposit_cat']

index_train <- createDataPartition(label, p = 0.8, list = FALSE)
data_train$deposit_cat <- label_train

data_train_no <- data_train %>% filter(deposit_cat == 0)
data_train_yes <- data_train %>% filter(deposit_cat == 1)


data_train_yes_over <- data_train_yes %>% sample_n(nrow(data_train_no), replace = TRUE)

data_train_balanced <- bind_rows(data_train_no, data_train_yes_over)

table(data_train_balanced$deposit_cat)

label_train <- data_train_balanced$deposit_cat
data_train <- data_train_balanced %>% select(-deposit_cat)


data_test <- data_drop_deposite[-index_train, ]
label_test <- label[-index_train]
```

### Function to train and evaluate a decision tree model

This function takes a depth parameter and trains a decision tree model using the rpart algorithm. Then evaluates the model's performance on a training set and a testing set by calculating the accuracy.

```{r}


train_and_evaluate <- function(depth) {
  model <- rpart(formula = deposit_cat ~ ., data = bankcl, method = "class",
                 control = rpart.control(maxdepth = depth, usesurrogate = 0, xval = 10))
  train_pred <- predict(model, data_train, type = "class")
  test_pred <- predict(model, data_test, type = "class")
  train_accuracy <- sum(train_pred == label_train) / length(label_train)
  test_accuracy <- sum(test_pred == label_test) / length(label_test)

  cat(sprintf("Depth %d: Training score: %f, Testing score: %f\n", depth, train_accuracy, test_accuracy))
  return(c(train = train_accuracy, test = test_accuracy)) 
}


```

### Compare Training and Testing scores for various tree depths used

```{r}
scores <- list()

depth_levels <- c(1, 2, 3, 4, 6)
names(depth_levels) <- c("max", "2", "3", "4", "6") 

for (depth in depth_levels) {
  result <- train_and_evaluate(depth)
  scores[[as.character(depth)]] <- list(train = result[1], test = result[2])
}

cat(sprintf('%-10s %-20s %-20s\n', 'depth', 'Training score', 'Testing score'))
cat(sprintf('%-10s %-20s %-20s\n', '-----', '--------------', '-------------'))
for (depth in c("2", "3", "4", "6", "max")) {
  cat(sprintf('%-1s %-25.2f %-20.2f\n', depth, scores[[depth]]$train, scores[[depth]]$test))
}
```

### Description:
This code fits a classification decision tree model using the rpart package in R. It uses the rpart() function to train the model on the 'data_train' dataset, with the target variable 'label_train' and all other variables as predictors. The model is limited to a maximum depth of 2 using the rpart.control() function. 

### Parameters:
 - label_train: The target variable for the classification model.
 - data_train: The training dataset containing the target variable and predictor variables.

### Output:
 - dt2: The trained decision tree model.
 - fi: The variable importance values calculated from the decision tree model.
 - features: The names of the predictor variables.
 - l: The length of the features vector.
 - The variable importance values and their corresponding feature names are printed using the cat() function.

```{r}
dt2 <- rpart(label_train ~ ., data = data_train, method = "class", control = rpart.control(maxdepth = 2))

fi <- dt2$variable.importance

features <- names(fi)
l <- length(features)
for (i in 1:l) {
  cat(sprintf('%-20s %3f\n', features[i], fi[i]))
}
```

### Prediction

```{r}
cat(sprintf("Mean duration   : %f\n", mean(data_drop_deposite$duration, na.rm = TRUE)))
cat(sprintf("Maximum duration: %d\n", max(data_drop_deposite$duration, na.rm = TRUE)))
cat(sprintf("Minimum duration: %d\n", min(data_drop_deposite$duration, na.rm = TRUE)))
```

```{r}
new_data_371 <- data.frame(matrix(0, nrow = 1, ncol = length(names(data_drop_deposite))))
names(new_data_371) <- names(data_drop_deposite)
new_data_371$duration <- 371  # Set duration for prediction

new_data_3881 <- data.frame(matrix(0, nrow = 1, ncol = length(names(data_drop_deposite))))
names(new_data_3881) <- names(data_drop_deposite)
new_data_3881$duration <- 3881  # Set duration for prediction


probabilities_371 <- predict(dt2, new_data_371, type = "prob")
probabilities_3881 <- predict(dt2, new_data_3881, type = "prob")

class_371 <- predict(dt2, new_data_371, type = "class")
class_3881 <- predict(dt2, new_data_3881, type = "class")

cat("Probabilities with duration 371 sec:\n")
print(probabilities_371)

cat("Predicted class with duration 371 sec:\n")
print(class_371)

cat("Probabilities with duration 3881 sec:\n")
print(probabilities_3881)

cat("Predicted class with duration 3881 sec:\n")
print(class_3881)
```

```{r}
specific_row <- data_drop_deposite[986, ]
```


```{r}
probability_prediction <- predict(dt2, specific_row, type = "prob")

print(probability_prediction)
```

```{r}
preds <- predict(dt2, data_test, type = "class")

levels_set <- union(levels(factor(label_test)), levels(factor(preds)))

preds_factor <- factor(preds, levels = levels_set)
label_test_factor <- factor(label_test, levels = levels_set)

accuracy_matrix <- confusionMatrix(preds_factor, label_test_factor)
accuracy <- accuracy_matrix$overall['Accuracy']

cat("\nAccuracy score: \n", accuracy)
```

```{r}
probs <- predict(dt2, data_test, type = "prob")


positive_probs <- probs[, 2]

auc_value <- roc(label_test, positive_probs)$auc
cat("\nArea Under Curve: \n", auc_value)
```
## Gradient Boosting

We employed the Gradient Boosting model because of its proficiency in handling various types of data features and its robustness in predicting binary outcomes

```{r}
gbm_model <- gbm(deposit_cat ~ ., 
                 data = bank_with_dummies[index_train, ], 
                 distribution = "bernoulli", 
                 n.trees = 200, 
                 interaction.depth = 4,
                 shrinkage = 0.01,
                 cv.folds = 5, 
                 n.minobsinnode = 10,
                 verbose = TRUE)


summary(gbm_model)

gbm.perf(gbm_model, method = "cv")

gbm_predictions_prob <- predict(gbm_model, data_test, n.trees = gbm_model$n.trees, type = "response")

gbm_predictions <- ifelse(gbm_predictions_prob > 0.5, 1, 0)

actual <- factor(label_test, levels = c(0, 1))
predicted <- factor(gbm_predictions, levels = c(0, 1))

conf_matrix <- confusionMatrix(predicted, actual)
conf_matrix

cat(sprintf("GBM Model Accuracy: %f\n", conf_matrix$overall['Accuracy']))

precision <- conf_matrix$byClass['Precision']
recall <- conf_matrix$byClass['Recall']
f1_score <- 2 * (precision * recall) / (precision + recall)

cat(sprintf("Precision: %f\n", precision))
cat(sprintf("Recall: %f\n", recall))
cat(sprintf("F1 Score: %f\n", f1_score))

roc_curve <- roc(actual, gbm_predictions_prob)
auc_value <- auc(roc_curve)

cat(sprintf("Area Under Curve (AUC): %f\n", auc_value))
```

```{r}

new_data_371 <- data.frame(matrix(0, nrow = 1, ncol = length(names(data_test))))
names(new_data_371) <- names(data_test)
new_data_371$duration <- 371  

new_data_3881 <- data.frame(matrix(0, nrow = 1, ncol = length(names(data_test))))
names(new_data_3881) <- names(data_test)
new_data_3881$duration <- 3881  

probabilities_371 <- predict(gbm_model, new_data_371, n.trees = gbm_model$n.trees, type = "response")
probabilities_3881 <- predict(gbm_model, new_data_3881, n.trees = gbm_model$n.trees, type = "response")

class_371 <- ifelse(probabilities_371 > 0.5, 1, 0)
class_3881 <- ifelse(probabilities_3881 > 0.5, 1, 0)

cat("Probabilities with duration 371 sec:\n")
print(probabilities_371)

cat("Predicted class with duration 371 sec:\n")
print(class_371)

cat("Probabilities with duration 3881 sec:\n")
print(probabilities_3881)

cat("Predicted class with duration 3881 sec:\n")
print(class_3881)

```
